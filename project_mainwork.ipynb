{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829e42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "hate_spotify_dataset = pd.read_csv(\"data/labeled_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c024ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = hate_spotify_dataset.lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c54f87",
   "metadata": {},
   "source": [
    "## Feature generation nach Davidson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca34766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "\n",
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = []\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) lots of whitespace with one instance\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(lyrics):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", lyrics.lower())).strip()\n",
    "    #tokens = re.split(\"[^a-zA-Z]*\", lyrics.lower())\n",
    "    tokens = [stemmer.stem(t) for t in lyrics.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(lyrics):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    lyrics = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", lyrics.lower())).strip()\n",
    "    return lyrics.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.501\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e684f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nina\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(lyrics).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e990407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efaa2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get POS tags for lyrics and save as a string\n",
    "lyrics_tags = []\n",
    "for l in lyrics:\n",
    "    tokens = basic_tokenize(preprocess(l))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    #for i in range(0, len(tokens)):\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    lyrics_tags.append(tag_str)\n",
    "        #print(tokens[i],tag_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a93df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.501,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbea4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(lyrics_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e49e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment\n",
    "#!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "130bdf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def other_features(lyrics):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(lyrics)\n",
    "    \n",
    "    words = preprocess(lyrics) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(lyrics)\n",
    "    num_terms = len(lyrics.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    \n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound']]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(lyrics):\n",
    "    feats=[]\n",
    "    for l in lyrics:\n",
    "        feats.append(other_features(l))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87ace76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \"vader compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed74750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_feature_array(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a69d4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61737465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766, 10543)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "304e3792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anmerkung: wir mussten items() statt iteritems() schreiben, weil es das nicht mehr in Python 3 gibt\n",
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf981b7",
   "metadata": {},
   "source": [
    "## Running the model nach Davidson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4de7cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = pd.DataFrame(M)\n",
    "y = hate_spotify_dataset['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5af6a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cea4ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anmerkung: wir haben zusätzlich solver=\"liblinear\" eingefügt, weil sonst L1 nicht unterstützt wird\n",
    "select = SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l1\",C=0.01, solver='liblinear'))\n",
    "X_ = select.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9c26a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nina\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',multi_class='ovr').fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4177dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "255dfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report( y, y_preds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "756ef47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.97      0.62       331\n",
      "           1       0.95      0.11      0.20       346\n",
      "           2       0.92      0.87      0.89      1089\n",
      "\n",
      "    accuracy                           0.74      1766\n",
      "   macro avg       0.78      0.65      0.57      1766\n",
      "weighted avg       0.84      0.74      0.71      1766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a0d365",
   "metadata": {},
   "source": [
    "# Using information from the model to obtain the matrix X_ generically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9149f9b",
   "metadata": {},
   "source": [
    "## Obtaining information about the model nach Davidson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e85ea296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] >= 3:\n",
    "    unicode = str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "951700e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = select.get_support(indices=True) #get indices of features\n",
    "final_feature_list = [unicode(feature_names[i]) for i in final_features] #Get list of names corresponding to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2456ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'em\", \"'fore\", '(a', '(hell', '(hey)', '(i', '(woah)', '(yeah!)', '(yeah)', '(you', '..', 'afraid', \"ain't\", 'ali', 'anoth one', 'are,', 'arm', 'around know', 'asham', 'ass', 'at,', 'at?', 'away,', 'ayi', 'ayy,', 'baby,', \"baby, i'm\", 'back', 'back ass', 'back never', 'bad,', 'bad, bad,', 'ball', 'baller', 'bands,', 'better', 'bitch', 'bitch better', 'bitch,', 'blood,', 'blue', 'boom', 'boom,', 'bop', 'bounc back', 'boy', 'brain', 'break', 'bring', 'buck,', 'bullet', 'bump', 'buy', 'california', 'camera', \"can't\", \"can't keep\", 'caus', 'chao', 'chick', 'chill', 'cocaine,', 'come aliv', 'cuz', 'da', \"dancin' like\", 'dawg', 'day,', 'dear', 'die', 'dig', 'dirti', 'do, do,', 'dolla', 'done', 'doo', 'dope', 'dream', 'drive crazi', 'drop', 'dumb', 'ear', 'east', 'eat', 'em', 'empir', 'ever', 'ever make', 'everi day', \"everi day i'm\", 'everi littl', 'everybodi', 'ex,', 'face', 'famili', 'feel', 'feel better', 'free', 'fuck', 'fuck)', \"fuckin'\", 'full', 'funki', 'gangsta', 'gat', 'get floor', 'get outta', \"gettin'\", 'girl', 'go,', 'god!', 'god,', \"gon'\", \"gon' let\", 'gone', 'gonna', 'goodbye,', 'gorgeou', 'got man', 'got money', 'gotta', 'gucci', 'hah,', 'hate', 'heart', 'hello,', 'hey,', 'hi', 'high,', 'hold back', 'home', 'hop', 'hot', 'hot girl', 'hot)', \"i'll\", \"i'm free\", 'i?', 'insid', \"it' like\", 'it)', 'it,', 'jacki', 'juic', 'keys,', 'kid', 'kill', 'knock', 'la', 'la la', 'la la la', 'la,', 'larg', 'lay', 'leav', 'left', 'let', 'lick', \"like i'm\", \"like it'\", \"lookin'\", 'love', 'love me?', 'love,', 'low', 'lust', 'mad', 'made', 'make move', 'make rain', 'mama', 'mama,', 'man', 'man,', 'man, know', 'mani', 'me,', 'mo', 'money', 'money,', 'money, get', 'monkey', 'moonlight', 'more,', \"mornin',\", 'mother', 'motherfuck', 'motherfucker,', 'murder', 'murder,', 'na na na', 'na,', 'neon', 'nigga', 'nigga hold', 'night like', 'no,', 'nobodi', 'now,', 'oh', 'oh oh', 'oh oh oh', 'oh yeah', 'oh,', 'okay,', 'old man', 'one night', 'onli', 'ooh,', 'open,', 'outsid', 'pain', 'peopl', 'picasso', 'pictur', 'play', \"poppin'\", \"prayin'\", 'psycho', 'pump', 'put', 'rack', 'ran', 'ring', 'road,', 'roll', \"rollin'\", 'rolling,', 'sad', 'said', 'save', 'scale', 'see get', 'shawti', \"she'\", 'shi', 'shit', 'shit,', 'shoot', 'short', 'shot', 'slay', 'slim', 'smoke', 'somebody,', \"somethin'\", 'squad', 'stay', 'stop,', 'sucker', 'sugar,', 'suicid', \"takin'\", 'tell', 'tha', \"that'\", \"that' like\", \"that' sound\", 'them,', 'there?', 'thi gangsta', 'thi shit', 'thing', 'thug', 'ti', 'till', 'time', 'tire', 'togeth', 'told ya', 'train', 'trap,', 'tray', 'turnt', 'two', 'u', 'uh,', 'uh, uh,', 'up)', 'wanna sleep', 'wanna,', 'want her,', 'wast', 'wave', 'way', \"we'r\", 'west', 'whi', 'whi love', 'whoa,', 'wick', 'wild,', 'wish wa', 'without', 'woman', 'woo', 'would', \"y'all\", 'ya,', 'yeah', 'yeah,', 'yeah, yeah,', 'yo', \"you'r\", 'you,', \"you, that'\", '–', '—', 'NN WP', 'FRE', 'num_syllables', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words']\n"
     ]
    }
   ],
   "source": [
    "print (final_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c613440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316\n"
     ]
    }
   ],
   "source": [
    "print(len(final_feature_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e775b",
   "metadata": {},
   "source": [
    "### Anmerkung: Wir müssen die jeweils letzten zugehörigen Elemente einer Feature-Gruppe ersetzen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5f25c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting names for each class of features\n",
    "ngram_features = final_feature_list[:final_feature_list.index(\"you, that'\")+1]\n",
    "pos_features = final_feature_list[final_feature_list.index(\"you, that'\")+1:final_feature_list.index('NN WP')+1]\n",
    "oth_features = final_feature_list[final_feature_list.index('NN WP')+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314708d5",
   "metadata": {},
   "source": [
    "## Generating ngram features nach Davidson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af8bc79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = {v:i for i, v in enumerate(ngram_features)}\n",
    "new_vocab_to_index = {}\n",
    "for k in ngram_features:\n",
    "    new_vocab_to_index[k] = vocab[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ada1261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get indices of text features\n",
    "ngram_indices = final_features[:len(ngram_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd0bc74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Pickle new vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fdafdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    vocabulary=new_vocab\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30f70fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_tfidf.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(new_vectorizer, 'final_tfidf.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e26dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nina\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "tfidf_ = new_vectorizer.fit_transform(lyrics).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "914a7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying that results are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a06e6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  7.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,\n",
       "        0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  7.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  2.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "        2.,  0.,  0.,  0.,  1.,  2.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        2.,  0.,  0.,  0.,  0.,  0.,  2.,  0., 18.,  0.,  1.,  0.,  0.,\n",
       "        0.,  0.,  2.,  0.,  0.,  0.,  4.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0., 23.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        6.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "498f43c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_[1,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5c6958a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766, 306)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26140459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.66836503,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  8.9969446 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , 13.51785905,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        2.60717547,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        3.65238667,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        4.41602937,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  8.02112853,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  4.86555447,  3.70578776,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  2.73990008,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       17.69906174,  0.        ,  0.        ,  2.90812788,  0.        ,\n",
       "        0.        ,  3.69734889,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        6.89206892,  9.0634538 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  3.60893793,  0.        ,\n",
       "        0.        ,  4.06150512,  0.        ,  0.        ,  0.        ,\n",
       "        3.75797351,  3.64463972,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  5.47980017,  0.        ,  0.        ,\n",
       "        3.88135253,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  2.14319275,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        4.38212782,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  2.39883014,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  3.12936485,  0.        ,  0.        ,\n",
       "        2.57111053,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  2.59036835,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        3.33480882,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  5.57222585,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  4.16911054,  0.        ,\n",
       "       36.15792579,  0.        ,  6.86703447,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  4.7567962 ,  0.        ,  0.        ,\n",
       "        0.        ,  7.45235265,  4.05763177,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  5.92608727,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  4.15898427,  0.        , 48.45114216,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , 13.24425658,\n",
       "        0.        ,  0.        ,  0.        ,  1.99183715,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,:tfidf_.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e50ff2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293.2405891929479"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,:tfidf_.shape[1]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d75d10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766, 316)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efdbf5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,:tfidf_.shape[1]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41e940",
   "metadata": {},
   "source": [
    "Results are the same if use IDF but the problem is that IDF will be different if we use different data. Instead we have to use the original IDF scores and multiply them by the new matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff060c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_vals_ = idf_vals[ngram_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95faaf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_vals_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eef38732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_idf.pkl']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Pickle idf_vals\n",
    "\n",
    "joblib.dump(idf_vals_, 'final_idf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565f574",
   "metadata": {},
   "source": [
    "### Zahl ändern! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b9530ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tfidf_[1,:]*idf_vals_) == X_[1,:306] #Got same value as final process array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cdb94a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_*idf_vals_ == X_[:,:306]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6dab35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidffinal = tfidf_*idf_vals_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9a30554a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766, 306)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidffinal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d97926",
   "metadata": {},
   "source": [
    "## Generating POS features nach Davidson\n",
    "This is simpler as we do not need to worry about IDF but it will be slower as we have to compute the POS tags for the new data. Here we can simply use the old POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "11ac4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pos = {v:i for i, v in enumerate(pos_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2923b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Pickle pos vectorizer\n",
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "new_pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    vocabulary=new_pos\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0223949f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_pos.pkl']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(new_pos_vectorizer, 'final_pos.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "86d53483",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ = new_pos_vectorizer.fit_transform(lyrics_tags).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d549ba0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766, 3)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a8758aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4de37",
   "metadata": {},
   "source": [
    "### Anmerkung: Haben wir hier die richtigen Zahlen eingetragen? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cef50bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 5.76842218, 1.        ])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,306:309]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "24edd4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True],\n",
       "       [ True, False,  True],\n",
       "       [ True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[:,:] == X_[:,306:309]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c74b0d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3152.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[:,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5e13510c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766, 3)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3b4dae45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3925.365896765484"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[:,306:309].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b336c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766, 316)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09d53e",
   "metadata": {},
   "source": [
    "## Finally, we can look at the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "446cf3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FKRA', 'FRE', 'num_syllables', 'avg_syl_per_word', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader neg', 'vader pos', 'vader neu', 'vader compound']\n"
     ]
    }
   ],
   "source": [
    "print (other_features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9888f7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FRE', 'num_syllables', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words']\n"
     ]
    }
   ],
   "source": [
    "print (oth_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14def520",
   "metadata": {},
   "source": [
    "The functions can be modified to only calculate and return necessary fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ec4e677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_features_(lyrics):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(lyrics)\n",
    "    \n",
    "    words = preprocess(lyrics) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(lyrics)\n",
    "    num_terms = len(lyrics.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    features = [FRE, syllables, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms]#, sentiment['compound']], FKRA is also removed, check the first feature list, you are not using it!\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array_(lyrics):\n",
    "    feats=[]\n",
    "    for l in lyrics:\n",
    "        feats.append(other_features_(l))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ddea80bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_ = get_feature_array_(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "18601a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-241.25,  411.  , 1731.  , 1731.  ,  341.  ,  341.  ,  199.  ])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8c1679f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-241.25,  411.  , 1731.  , ...,  341.  ,  341.  ,  199.  ],\n",
       "       [-621.85,  818.  , 3556.  , ...,  722.  ,  722.  ,  308.  ],\n",
       "       [-426.5 ,  608.  , 2592.  , ...,  528.  ,  528.  ,  284.  ],\n",
       "       ...,\n",
       "       [-435.7 ,  592.  , 2603.  , ...,  542.  ,  542.  ,  291.  ],\n",
       "       [-679.14,  902.  , 3949.  , ...,  776.  ,  776.  ,  415.  ],\n",
       "       [-521.59,  682.  , 2810.  , ...,  627.  ,  627.  ,  227.  ]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "443469f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766, 7)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "adb1c2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-241.25,  411.  , 1731.  , 1731.  ,  341.  ,  341.  ,  199.  ])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0,309:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cdc09d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766, 316)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d7ab45fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_[:,:] == X_[:,309:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393858fc",
   "metadata": {},
   "source": [
    "## Now that we have put it all together using a simplified process we can assess if these new data return the same answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "82fcac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_ = np.concatenate([tfidffinal, pos_, feats_],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16bcc5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766, 316)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2b73eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X__ = pd.DataFrame(M_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "222f9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_ = model.predict(X__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c6006c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report( y, y_preds_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "35e78a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.97      0.62       331\n",
      "           1       0.93      0.11      0.20       346\n",
      "           2       0.92      0.87      0.89      1089\n",
      "\n",
      "    accuracy                           0.74      1766\n",
      "   macro avg       0.77      0.65      0.57      1766\n",
      "weighted avg       0.84      0.74      0.71      1766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
